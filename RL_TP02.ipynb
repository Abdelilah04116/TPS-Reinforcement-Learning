{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Font size='6pt' color='blue'><center>RL_TP02<center/></Font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùÑÔ∏è TP - Exploration du Q-Learning avec FrozenLake\n",
    "\n",
    "#### üéØ Objectif du TP\n",
    "L'objectif de ce TP est de mettre en pratique les concepts fondamentaux de l'**apprentissage par renforcement** √† travers l‚Äôalgorithme **Q-Learning**.\n",
    "\n",
    "#### üìå Contenu du TP\n",
    "Au fil d‚Äôexercices progressifs, les √©tudiants vont :\n",
    "\n",
    "1. üî® **Impl√©menter l‚Äôalgorithme Q-Learning**\n",
    "   - Comprendre les √©tapes de mise √† jour de la **Q-Table**.\n",
    "\n",
    "2. ‚öôÔ∏è **Analyser les strat√©gies d'exploration et d'exploitation**\n",
    "   - Observer l‚Äôimpact du **taux d‚Äôexploration (Œµ)** et de l'**exploitation** sur l‚Äôapprentissage.\n",
    "\n",
    "3. üìà **√âtudier la convergence des valeurs Q**\n",
    "   - Visualiser comment les valeurs Q √©voluent et convergent vers une **politique optimale**.\n",
    "\n",
    "#### üåç Environnement d'exp√©rimentation\n",
    "L‚Äôenvironnement **FrozenLake** de **OpenAI Gym** sera utilis√© comme terrain d‚Äôexp√©rimentation.  \n",
    "Il permettra d'illustrer concr√®tement **comment un agent apprend √† optimiser ses d√©cisions** en fonction des mises √† jour successives de sa **Q-Table**.\n",
    "\n",
    "---\n",
    "\n",
    "üöÄ √Ä la fin de ce TP, vous serez capables de :  \n",
    "‚úÖ Impl√©menter un agent Q-Learning  \n",
    "‚úÖ Analyser son comportement d‚Äôapprentissage  \n",
    "‚úÖ Ajuster les param√®tres pour am√©liorer ses performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Exploration de l'Environnement FrozenLake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'√©tats: Discrete(16)\n",
      "Espace d'actions: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# Charger l‚Äôenvironnement FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\")\n",
    "print(\"Espace d'√©tats:\", env.observation_space)\n",
    "print(\"Espace d'actions:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpisode 1, √âtat: 0, Action: 1, R√©compense: 0.0\n",
      "√âpisode 1, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 1, √âtat: 1, Action: 2, R√©compense: 0.0\n",
      "√âpisode 2, √âtat: 0, Action: 1, R√©compense: 0.0\n",
      "√âpisode 2, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 2, √âtat: 0, Action: 1, R√©compense: 0.0\n",
      "√âpisode 2, √âtat: 4, Action: 0, R√©compense: 0.0\n",
      "√âpisode 2, √âtat: 8, Action: 1, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 0, Action: 0, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 4, Action: 3, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 4, Action: 3, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 0, Action: 1, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 3, √âtat: 1, Action: 1, R√©compense: 0.0\n",
      "√âpisode 4, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 4, √âtat: 4, Action: 0, R√©compense: 0.0\n",
      "√âpisode 4, √âtat: 4, Action: 0, R√©compense: 0.0\n",
      "√âpisode 4, √âtat: 0, Action: 3, R√©compense: 0.0\n",
      "√âpisode 4, √âtat: 0, Action: 1, R√©compense: 0.0\n",
      "√âpisode 4, √âtat: 4, Action: 3, R√©compense: 0.0\n",
      "√âpisode 5, √âtat: 0, Action: 2, R√©compense: 0.0\n",
      "√âpisode 5, √âtat: 1, Action: 3, R√©compense: 0.0\n",
      "√âpisode 5, √âtat: 1, Action: 2, R√©compense: 0.0\n",
      "√âpisode 5, √âtat: 2, Action: 2, R√©compense: 0.0\n",
      "√âpisode 5, √âtat: 3, Action: 0, R√©compense: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Ex√©cution d'une boucle d'interaction al√©atoire\n",
    "num_episodes = 5\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Choix al√©atoire d'une action\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        print(f\"√âpisode {episode+1}, √âtat: {state}, Action: {action}, R√©compense: {reward}\")\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Impl√©mentation de la Q-Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table initiale :\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q_table = np.zeros((num_states, num_actions))\n",
    "print(\"\\nQ-Table initiale :\\n\", q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Impl√©mentation du Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 \n",
    "gamma = 0.99  # Facteur de discount\n",
    "epsilon = 1.0  # Probabilit√© d'exploration\n",
    "epsilon_decay = 0.99  # R√©duction progressive d'epsilon\n",
    "epsilon_min = 0.01  # Valeur minimale d'epsilon\n",
    "num_episodes = 5000  # Nombre d'√©pisodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table apr√®s apprentissage :\n",
      " [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample() \n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) \n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        q_table[state, action] = q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[state, action])\n",
    "        state = next_state\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "print(\"\\nQ-Table apr√®s apprentissage :\\n\", q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **√âvaluation des Performances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Taux de r√©ussite apr√®s apprentissage : 0/100 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])  \n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        if reward == 1:\n",
    "            successes += 1\n",
    "print(f\"\\nTaux de r√©ussite apr√®s apprentissage : {successes}/{num_test_episodes} ({(successes/num_test_episodes)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
